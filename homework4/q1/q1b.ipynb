{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if GPU is available\n",
    "Please note that GPU will be required to train your model here. If you are running this in gsds server, follow the instruction below. You will have to use a command \"launch-jupyter\" to run jupyter notebook with GPU.\n",
    "\n",
    "For those of you who received gsds server accounts for this class, use \"class2\" for your slurm partition.\n",
    "\n",
    "https://gsds.gitbook.io/gsds/for-beginners/slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Requried Packages if not installed in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell before you start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration, Pix2StructProcessor\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-tune a pre-trained Image-to-Text model for Image Captioning\n",
    "\n",
    "In this problem, you will load a pre-trained model, *pix2struct*, and fine-tune it to perform an image captioning task. \n",
    "\n",
    "Pix2Struct is a pretrained image-to-text model for visual language understanding, and it has been fine-tuned on a variety of tasks and datasets, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. For more details about this model, please refer to the paper [here](https://arxiv.org/abs/2210.03347).\n",
    "\n",
    "We will use a image captioning dataset from Hugging Face to fine-tune the model. The dataset contains Pokémon images with captions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Load the image captioning dataset. Split the data into training and validation set. **(4 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"lambdalabs/pokemon-blip-captions\"\n",
    "\n",
    "### YOUR CODE HERE (~4 lines)\n",
    "### TODO:\n",
    "### 1. Load dataset from the given data_path\n",
    "### 2. Spit the dataset into train and validation set (10% will be used for our valid set) and save in 'train_set' and 'valid_set'\n",
    "### Hint: use load_dataset and train_test_split. You can refer to the following document for 'datasets' library\n",
    "### https://pypi.org/project/datasets/\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this cell, you will see a sample data in your training set.\n",
    "print(train_set[0][\"text\"])\n",
    "plt.imshow(train_set[0][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this cell, you will see a sample data in your validation set.\n",
    "\n",
    "print(valid_set[0][\"text\"])\n",
    "plt.imshow(valid_set[0][\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "DO NOT modify the code below for data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATCHES = 16\n",
    "\n",
    "class ICDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], return_tensors=\"pt\", \n",
    "                                  add_special_tokens=True, max_patches=MAX_PATCHES)\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        encoding[\"text\"] = item[\"text\"]\n",
    "        return encoding\n",
    "\n",
    "\n",
    "class my_collate(object):\n",
    "    def __init__(self, processor, max_length):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "       new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "       texts = [item[\"text\"] for item in batch]\n",
    "\n",
    "       text_inputs = self.processor(text=texts, padding=\"max_length\", return_tensors=\"pt\", \n",
    "                                add_special_tokens=True, max_length=self.max_length)\n",
    "       new_batch[\"labels\"] = text_inputs.input_ids\n",
    "\n",
    "       for item in batch:\n",
    "          new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n",
    "          new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "    \n",
    "       new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n",
    "       new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n",
    "\n",
    "       return new_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT modify below hyperparameters. \n",
    "max_length = 20\n",
    "batch_size = 2\n",
    "model_path = \"google/pix2struct-base\"\n",
    "\n",
    "processor = Pix2StructProcessor.from_pretrained(model_path)\n",
    "my_collate_fn = my_collate(processor, max_length)\n",
    "\n",
    "train_dataset = ICDataset(train_set, processor)\n",
    "valid_dataset = ICDataset(valid_set, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=my_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, collate_fn=my_collate_fn)\n",
    "\n",
    "sample_data = next(iter(train_loader))\n",
    "print(\"Total number of training data :\", len(train_loader.dataset))\n",
    "print(\"Total number of test data :\", len(valid_loader.dataset))\n",
    "print(\"Flattened patches size: \", sample_data['flattened_patches'].size())\n",
    "print(\"Attention mask size: \", sample_data['attention_mask'].size())\n",
    "print(\"Label size: \", sample_data['labels'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and prepare for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT modify below hyperparameters. \n",
    "LEARNING_RATE = 1e-5\n",
    "model_path = \"google/pix2struct-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "print(\"Device :\", device)   # cuda\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Next, we will fine-tune the model to our dataset. Complete the train and evaluation code following the instruction in jupyter notebook file. It will take about 1 hour to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Complete the training code in `train()` function of Trainer class. **(10 pts)**\n",
    "\n",
    "**(c)** Complete the validation code in `train()` function of Trainer class. **(6 pts)** \n",
    "\n",
    "**(d)** After the training is complete, use the `plot()` function of Trainer class to display the figure, and then paste it to your latex file. **(6 pts)** \n",
    "\n",
    "**(e)** Based on your training results in (d), evaluate whether the training was successful, and write at least two ways to improve the model’s performance. **(4 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "  def __init__(self, model, train_loader, valid_loader, optimizer):\n",
    "    self.model = model\n",
    "    self.train_loader = train_loader\n",
    "    self.valid_loader = valid_loader\n",
    "    self.optimizer = optimizer\n",
    "    self.epoch_train_loss, self.epoch_val_loss = [], []\n",
    "\n",
    "  def train(self, epochs):\n",
    "    if self.epoch_train_loss:\n",
    "      self.epoch_train_loss, self.epoch_val_loss = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      ########### TRAIN ###########\n",
    "      self.model.train()\n",
    "      train_losses = []\n",
    "      for i, batch in enumerate(self.train_loader):\n",
    "        ### YOUR CODE HERE (~9 lines)\n",
    "        ### TODO:\n",
    "        ###     1. Perform a forward pass on the loaded model.\n",
    "        ###     2. Calculate the loss and perform backpropagation.\n",
    "        ###     3. Save your loss in train_loss in float type.\n",
    "        ###     4. Update the model's parameters using the optimizer.\n",
    "        ### Hint: for input parameters of the model, refer to the below link. you can also check what's included in your batch in above cells.\n",
    "        ###       https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration\n",
    "        ### Hint: we can compute loss as we have 'labels' in our batch.\n",
    "         \n",
    "\n",
    "        ### END YOUR CODE\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "      ######### VALIDATION #########\n",
    "      val_losses = []\n",
    "      self.model.eval()\n",
    "      for i, batch in enumerate(self.valid_loader):\n",
    "        ### YOUR CODE HERE (~6 lines)\n",
    "        ### TODO:\n",
    "        ###     1. Perform a forward pass on the loaded model. \n",
    "        ###     2. Calculate the loss and save your loss in val_loss in float type.\n",
    "        ### Hint: use the same approach you made in the previous question to get output and loss\n",
    "\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "      ### DO NOT modify below codes\n",
    "      train_loss = np.mean(train_losses)\n",
    "      val_loss = np. mean(val_losses)\n",
    "      self.epoch_train_loss.append(train_loss)\n",
    "      self.epoch_val_loss.append(val_loss)\n",
    "      print(f'Epoch {epoch+1}')\n",
    "      print(f'train_loss : {train_loss} val_loss : {val_loss}')\n",
    "\n",
    "  def plot(self):\n",
    "    fig = plt.figure(figsize = (20, 6))\n",
    "    plt.plot(self.epoch_train_loss, label='Train loss')\n",
    "    plt.plot(self.epoch_val_loss, label='Validation loss')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "  def inference(self, image, processor):\n",
    "    MAX_PATCHES=16\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    encoding = processor(images=image, return_tensors=\"pt\", add_special_tokens=True, max_patches=MAX_PATCHES)\n",
    "    encoding = encoding.to(device)\n",
    "\n",
    "    flattened_patches = encoding['flattened_patches']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    generated_ids = model.generate(flattened_patches=flattened_patches, attention_mask=attention_mask)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_caption    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "trainer= Trainer(model=model,\n",
    "                 train_loader=train_loader,\n",
    "                 valid_loader=valid_loader,\n",
    "                 optimizer=optimizer)\n",
    "trainer.train(EPOCHS)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with your fine-tuned model\n",
    "\n",
    "Load the image that you want to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_caption = trainer.inference(image, processor)\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
